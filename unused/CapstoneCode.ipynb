{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TY0X6iq7SDwe"
   },
   "source": [
    "# Kevin Ferdinand 0358519 Capstone Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5h-Q1UAz-jQb"
   },
   "source": [
    "## Import Modules and Install PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27H-8yGJmmfx",
    "outputId": "3d49ef53-4f92-4731-ab37-3443ca3f8b0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- --------\n",
      "absl-py                 1.3.0\n",
      "aiohttp                 3.8.3\n",
      "aiosignal               1.2.0\n",
      "altair                  4.1.0\n",
      "anyio                   3.5.0\n",
      "appdirs                 1.4.4\n",
      "argon2-cffi             21.3.0\n",
      "argon2-cffi-bindings    21.2.0\n",
      "asttokens               2.0.5\n",
      "astunparse              1.6.3\n",
      "async-timeout           4.0.2\n",
      "attrs                   22.1.0\n",
      "autovizwidget           0.20.0\n",
      "Babel                   2.11.0\n",
      "backcall                0.2.0\n",
      "beautifulsoup4          4.12.2\n",
      "bleach                  4.1.0\n",
      "blinker                 1.4\n",
      "Bottleneck              1.3.5\n",
      "brotlipy                0.7.0\n",
      "cachetools              4.2.2\n",
      "certifi                 2023.5.7\n",
      "cffi                    1.15.1\n",
      "charset-normalizer      2.0.4\n",
      "click                   8.0.4\n",
      "colorama                0.4.6\n",
      "comm                    0.1.2\n",
      "contourpy               1.0.5\n",
      "cryptography            39.0.1\n",
      "cycler                  0.11.0\n",
      "debugpy                 1.5.1\n",
      "decorator               5.1.1\n",
      "defusedxml              0.7.1\n",
      "docopt                  0.6.2\n",
      "entrypoints             0.4\n",
      "executing               0.8.3\n",
      "fastavro                1.7.0\n",
      "fastjsonschema          2.16.2\n",
      "findpython              0.2.2\n",
      "flatbuffers             2.0\n",
      "fonttools               4.25.0\n",
      "frozenlist              1.3.3\n",
      "gast                    0.4.0\n",
      "gitdb                   4.0.7\n",
      "GitPython               3.1.30\n",
      "google-auth             2.6.0\n",
      "google-auth-oauthlib    0.4.4\n",
      "google-pasta            0.2.0\n",
      "graphviz                0.20.1\n",
      "grpcio                  1.48.2\n",
      "h5py                    3.7.0\n",
      "hdfs                    2.7.0\n",
      "hdijupyterutils         0.20.0\n",
      "idna                    3.4\n",
      "importlib-metadata      6.0.0\n",
      "importlib-resources     5.2.0\n",
      "ipykernel               6.19.2\n",
      "ipython                 8.12.0\n",
      "ipython-genutils        0.2.0\n",
      "ipywidgets              8.0.4\n",
      "jedi                    0.18.1\n",
      "Jinja2                  3.1.2\n",
      "joblib                  1.2.0\n",
      "json5                   0.9.6\n",
      "jsonschema              4.17.3\n",
      "jupyter                 1.0.0\n",
      "jupyter_client          8.1.0\n",
      "jupyter-console         6.6.3\n",
      "jupyter_core            5.3.0\n",
      "jupyter-server          1.23.4\n",
      "jupyterlab              3.5.3\n",
      "jupyterlab-pygments     0.1.2\n",
      "jupyterlab_server       2.22.0\n",
      "jupyterlab-widgets      3.0.5\n",
      "keras                   2.10.0\n",
      "Keras-Preprocessing     1.1.2\n",
      "kiwisolver              1.4.4\n",
      "lxml                    4.9.2\n",
      "Markdown                3.4.1\n",
      "markdown-it-py          2.2.0\n",
      "MarkupSafe              2.1.1\n",
      "matplotlib              3.7.1\n",
      "matplotlib-inline       0.1.6\n",
      "mdurl                   0.1.0\n",
      "mistune                 0.8.4\n",
      "mkl-fft                 1.3.6\n",
      "mkl-random              1.2.2\n",
      "mkl-service             2.4.0\n",
      "mock                    4.0.3\n",
      "multidict               6.0.2\n",
      "munkres                 1.1.4\n",
      "nbclassic               0.5.5\n",
      "nbclient                0.5.13\n",
      "nbconvert               6.5.4\n",
      "nbformat                5.7.0\n",
      "nest-asyncio            1.5.5\n",
      "nose                    1.3.7\n",
      "notebook                6.5.4\n",
      "notebook_shim           0.2.2\n",
      "numexpr                 2.8.4\n",
      "numpy                   1.24.3\n",
      "oauthlib                3.2.2\n",
      "opt-einsum              3.3.0\n",
      "packaging               23.0\n",
      "pandas                  1.5.3\n",
      "pandocfilters           1.5.0\n",
      "parso                   0.8.3\n",
      "pickleshare             0.7.5\n",
      "Pillow                  9.4.0\n",
      "pip                     23.0.1\n",
      "platformdirs            2.5.2\n",
      "plotly                  5.9.0\n",
      "ply                     3.11\n",
      "pooch                   1.4.0\n",
      "prometheus-client       0.14.1\n",
      "prompt-toolkit          3.0.36\n",
      "protobuf                3.20.3\n",
      "psutil                  5.9.0\n",
      "pure-eval               0.2.2\n",
      "py4j                    0.10.9.3\n",
      "pyarrow                 11.0.0\n",
      "pyasn1                  0.4.8\n",
      "pyasn1-modules          0.2.8\n",
      "pycparser               2.21\n",
      "pydeck                  0.7.1\n",
      "pydot                   1.4.1\n",
      "pydotplus               2.0.2\n",
      "Pygments                2.15.1\n",
      "pygraphviz              1.9\n",
      "PyJWT                   2.4.0\n",
      "Pympler                 0.9\n",
      "pyOpenSSL               23.0.0\n",
      "pyparsing               3.0.9\n",
      "PyQt5                   5.15.7\n",
      "PyQt5-sip               12.11.0\n",
      "pyrsistent              0.18.0\n",
      "PySocks                 1.7.1\n",
      "pyspark                 3.2.1\n",
      "python-dateutil         2.8.2\n",
      "pytz                    2022.7\n",
      "pywin32                 305.1\n",
      "pywinpty                2.0.10\n",
      "PyYAML                  6.0\n",
      "pyzmq                   25.0.2\n",
      "qtconsole               5.4.2\n",
      "QtPy                    2.2.0\n",
      "regex                   2022.7.9\n",
      "requests                2.29.0\n",
      "requests-kerberos       0.12.0\n",
      "requests-oauthlib       1.3.0\n",
      "rich                    13.3.5\n",
      "rsa                     4.7.2\n",
      "scikit-learn            1.2.2\n",
      "scipy                   1.10.1\n",
      "seaborn                 0.12.2\n",
      "semver                  2.13.0\n",
      "Send2Trash              1.8.0\n",
      "setuptools              67.8.0\n",
      "sip                     6.6.2\n",
      "six                     1.16.0\n",
      "smmap                   4.0.0\n",
      "sniffio                 1.2.0\n",
      "soupsieve               2.4\n",
      "sparkmagic              0.20.0\n",
      "stack-data              0.2.0\n",
      "streamlit               1.16.0\n",
      "tenacity                8.2.2\n",
      "tensorboard             2.10.0\n",
      "tensorboard-data-server 0.6.1\n",
      "tensorboard-plugin-wit  1.8.1\n",
      "tensorflow              2.10.0\n",
      "tensorflow-estimator    2.10.0\n",
      "termcolor               2.1.0\n",
      "terminado               0.17.1\n",
      "threadpoolctl           2.2.0\n",
      "tinycss2                1.2.1\n",
      "toml                    0.10.2\n",
      "tomli                   2.0.1\n",
      "toolz                   0.12.0\n",
      "tornado                 6.2\n",
      "traitlets               5.7.1\n",
      "typing_extensions       4.5.0\n",
      "tzlocal                 2.1\n",
      "urllib3                 1.26.15\n",
      "validators              0.18.2\n",
      "watchdog                2.1.6\n",
      "wcwidth                 0.2.5\n",
      "webencodings            0.5.1\n",
      "websocket-client        0.58.0\n",
      "Werkzeug                2.2.3\n",
      "wheel                   0.38.4\n",
      "widgetsnbextension      4.0.5\n",
      "win-inet-pton           1.1.0\n",
      "winkerberos             0.9.1\n",
      "wrapt                   1.14.1\n",
      "yarl                    1.8.1\n",
      "zipp                    3.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uz7IOjDxz7DO",
    "outputId": "912f7add-4ee1-4b36-92bf-362e333c2df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\kevin\\anaconda3\\envs\\capstone\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in c:\\users\\kevin\\anaconda3\\envs\\capstone\\lib\\site-packages (from pyspark) (0.10.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IsieF6A30HRj",
    "outputId": "f892c599-9abe-4a0d-8000-0d9d52bfacde"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType, StringType, FloatType\n",
    "from pyspark.sql.functions import when, expr\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.tree import GradientBoostedTrees\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col,count, explode, array, lit,mean, sum as spark_sum\n",
    "from pyspark.ml import PipelineModel\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDzywWbU-qkW"
   },
   "source": [
    "## Create SparkSession and SparkContext Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNBHMjin0HTz"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').appName('solution').config('spark.executor.memory', '8g').config('spark.driver.memory', '8g').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSdGMc3l-tix"
   },
   "source": [
    "### Load Data as RDDs - txt to Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrHvFuY80HYG"
   },
   "outputs": [],
   "source": [
    "# Load Each Txt File and use spark to convert it into a Pipeline RDD\n",
    "loanOriginationRDD2017 = sc.textFile(data_dir+'sample_orig_2017.txt').map(lambda x: x.split('|'))\n",
    "loanOriginationRDD2018 = sc.textFile(data_dir+'sample_orig_2018.txt').map(lambda x: x.split('|'))\n",
    "loanOriginationRDD2019 = sc.textFile(data_dir+'sample_orig_2019.txt').map(lambda x: x.split('|'))\n",
    "loanOriginationRDD2020 = sc.textFile(data_dir+'sample_orig_2020.txt').map(lambda x: x.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4dpX_8D1GDB"
   },
   "outputs": [],
   "source": [
    "#loanOriginationRDD = loanOriginationRDD2017.union(loanOriginationRDD2018).union(loanOriginationRDD2019).union(loanOriginationRDD2020)\n",
    "loanOriginationRDD = loanOriginationRDD2018.union(loanOriginationRDD2019).union(loanOriginationRDD2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soQgPiIE0HaX"
   },
   "outputs": [],
   "source": [
    "monthlyPerformanceRDD2017 = sc.textFile(data_dir+'sample_svcg_2017.txt').map(lambda x: x.split('|'))\n",
    "monthlyPerformanceRDD2018 = sc.textFile(data_dir+'sample_svcg_2018.txt').map(lambda x: x.split('|'))\n",
    "monthlyPerformanceRDD2019 = sc.textFile(data_dir+'sample_svcg_2019.txt').map(lambda x: x.split('|'))\n",
    "monthlyPerformanceRDD2020 = sc.textFile(data_dir+'sample_svcg_2020.txt').map(lambda x: x.split('|'))\n",
    "\n",
    "#monthlyPerformanceRDD = monthlyPerformanceRDD2017.union(monthlyPerformanceRDD2018).union(monthlyPerformanceRDD2019).union(monthlyPerformanceRDD2020)\n",
    "monthlyPerformanceRDD = monthlyPerformanceRDD2018.union(monthlyPerformanceRDD2019).union(monthlyPerformanceRDD2020)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQf13PDJ_Pu0"
   },
   "source": [
    "### Check Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XNh3dWl0Hcr",
    "outputId": "e01f78f5-a3af-471e-a203-66981f87a6d0"
   },
   "outputs": [],
   "source": [
    "print('There are', loanOriginationRDD.count(), 'records in the loan origination dataset.')\n",
    "print('There are', monthlyPerformanceRDD.count(), 'records in the monthly performance dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFD8SGzF_RjC"
   },
   "source": [
    "## Convert it into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBOhBzZ10HfN"
   },
   "outputs": [],
   "source": [
    "# Create the Loan Origination DataFrame from the Loan Origination RDD.\n",
    "# Next, assign new column names to that DataFrame.\n",
    "loanOriginationDF = spark.createDataFrame(loanOriginationRDD).toDF(\n",
    "    'CREDIT_SCORE', \n",
    "    'FIRST_PAYMENT_DATE',\n",
    "    'FIRST_TIME_HOMEBUYER_FLAG',\n",
    "    'MATURITY_DATE',\n",
    "    'METROPOLITAN_STATISTICAL_AREA',\n",
    "    'MORTGAGE_INSURANCE_PERCENTAGE',\n",
    "    'NUMBER_OF_UNITS',\n",
    "    'OCCUPANCY_STATUS',\n",
    "    'ORIGINAL_COMBINED_LOAN_TO_VALUE',\n",
    "    'ORIGINAL_DEBT_TO_INCOME_RATIO',\n",
    "    'ORIGINAL_UPB',\n",
    "    'ORIGINAL_LOAN_TO_VALUE',\n",
    "    'ORIGINAL_INTEREST_RATE',\n",
    "    'CHANNEL',\n",
    "    'PREPAYMENT_PENALTY_MORTGAGE_FLAG',\n",
    "    'AMORTIZATION_TYPE',\n",
    "    'PROPERTY_STATE',\n",
    "    'PROPERTY_TYPE',\n",
    "    'POSTAL_CODE',\n",
    "    'LOAN_SEQUENCE_NUMBER',\n",
    "    'LOAN_PURPOSE',\n",
    "    'ORIGINAL_LOAN_TERM',\n",
    "    'NUMBER_OF_BORROWERS',\n",
    "    'SELLER_NAME',\n",
    "    'SERVICER_NAME',\n",
    "    'SUPER_CONFORMING_FLAG',\n",
    "    'PRERELIEF_REFINANCE_LOAN_SEQUENCE_NUMBER',\n",
    "    'PROGRAM_INDICATOR',\n",
    "    'RELIEF_REFINANCE_INDICATOR',\n",
    "    'PROPERTY_VALUATION_METHOD',\n",
    "    'INTEREST_ONLY_INDICATOR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAaQtJro_Xg7"
   },
   "source": [
    "### Extract DELINQUENT from Monthly Performance RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THoqgEiJ0Hhr",
    "outputId": "08d31a41-eba0-420d-985a-11705d89143e"
   },
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "# Txt File states that 0 = no delinquency , 1 = have a delinquent \n",
    "# Extract these coreesponding to the Loan Sequence Number and check if there is any instance of delinquency\n",
    "def fillDictionary(record):\n",
    "    loanSequenceNumber = record[0]\n",
    "    currentLoanDelinquencyStatus = record[3]\n",
    "    delinquent = 0 if currentLoanDelinquencyStatus == '0' else 1\n",
    "    if loanSequenceNumber in dictionary:\n",
    "        if dictionary[loanSequenceNumber] == 0:\n",
    "            dictionary[loanSequenceNumber] = delinquent\n",
    "    else:\n",
    "        dictionary[loanSequenceNumber] = delinquent\n",
    "\n",
    "monthlyPerformanceData = monthlyPerformanceRDD.collect()\n",
    "\n",
    "for record in monthlyPerformanceData:\n",
    "    fillDictionary(record)\n",
    "print('The dictionary contains', len(dictionary), 'key-value pairs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k29VFYZAZda"
   },
   "source": [
    "### Create Delinquent DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCa9FNY00HkC",
    "outputId": "41bc5a5f-916d-4362-8814-1de8e8b114de"
   },
   "outputs": [],
   "source": [
    "delinquentDF = spark.createDataFrame(dictionary.items(), schema=['LOAN_SEQUENCE_NUMBER', 'DELINQUENT'])\n",
    "delinquentDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-wpAx7NYHbs"
   },
   "source": [
    "Now Join them with the other file to get a pair of person data and delinquency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3f36iCp0HmJ",
    "outputId": "ce290a28-f9e5-4969-aa04-148f80ef8fbe"
   },
   "outputs": [],
   "source": [
    "df = loanOriginationDF.join(delinquentDF, 'LOAN_SEQUENCE_NUMBER')  # inner join\n",
    "\n",
    "loanInDelinquencyCount = df.filter(df['DELINQUENT'] == 1).count()\n",
    "loanNotInDelinquencyCount = df.filter(df['DELINQUENT'] == 0).count()\n",
    "loanCount = loanInDelinquencyCount + loanNotInDelinquencyCount\n",
    "\n",
    "print('There are', loanInDelinquencyCount, 'loans in delinquency.')\n",
    "print('There are', loanNotInDelinquencyCount, 'loans not in delinquency.')\n",
    "print('There are', loanCount, 'available loans in total.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tfWs4QfYQe8"
   },
   "source": [
    "### Drop Columns that is not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsToDrop = [\n",
    "    'FIRST_PAYMENT_DATE',\n",
    "    'MATURITY_DATE',\n",
    "    'MORTGAGE_INSURANCE_PERCENTAGE',\n",
    "    'ORIGINAL_UPB',\n",
    "    'PREPAYMENT_PENALTY_MORTGAGE_FLAG',\n",
    "    'PROPERTY_STATE',\n",
    "    'LOAN_SEQUENCE_NUMBER',\n",
    "    'SELLER_NAME',\n",
    "    'SERVICER_NAME',\n",
    "    'SUPER_CONFORMING_FLAG',\n",
    "    'PRERELIEF_REFINANCE_LOAN_SEQUENCE_NUMBER',\n",
    "    'PROGRAM_INDICATOR',\n",
    "    'RELIEF_REFINANCE_INDICATOR',\n",
    "    'PROPERTY_VALUATION_METHOD',\n",
    "    'INTEREST_ONLY_INDICATOR',\n",
    "    'AMORTIZATION_TYPE'\n",
    "]\n",
    "\n",
    "df = df.drop(*columnsToDrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_g48RwZjacX9",
    "outputId": "cc9157b1-50a9-441d-c9c7-1aa62da456b4"
   },
   "outputs": [],
   "source": [
    "# Check Datatypes\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmWWlGK-acpt"
   },
   "source": [
    "Change some Datatypes to Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCF47R9K0Hqi",
    "outputId": "acf957e5-f886-427d-8bfd-ff304b951565"
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('CREDIT_SCORE', df['CREDIT_SCORE'].cast(DoubleType()))\n",
    "df = df.withColumn('METROPOLITAN_STATISTICAL_AREA', df['METROPOLITAN_STATISTICAL_AREA'].cast(DoubleType()))\n",
    "df = df.withColumn('NUMBER_OF_UNITS', df['NUMBER_OF_UNITS'].cast(DoubleType()))\n",
    "df = df.withColumn('ORIGINAL_COMBINED_LOAN_TO_VALUE', df['ORIGINAL_COMBINED_LOAN_TO_VALUE'].cast(DoubleType()))\n",
    "df = df.withColumn('ORIGINAL_DEBT_TO_INCOME_RATIO', df['ORIGINAL_DEBT_TO_INCOME_RATIO'].cast(DoubleType()))\n",
    "df = df.withColumn('ORIGINAL_LOAN_TO_VALUE', df['ORIGINAL_LOAN_TO_VALUE'].cast(DoubleType()))\n",
    "df = df.withColumn('POSTAL_CODE', df['POSTAL_CODE'].cast(DoubleType()))\n",
    "df = df.withColumn('ORIGINAL_LOAN_TERM', df['ORIGINAL_LOAN_TERM'].cast(DoubleType()))\n",
    "df = df.withColumn('NUMBER_OF_BORROWERS', df['NUMBER_OF_BORROWERS'].cast(DoubleType()))\n",
    "df = df.withColumn('ORIGINAL_INTEREST_RATE', df['ORIGINAL_INTEREST_RATE'].cast(DoubleType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullCountDF = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullCountDF.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3smw1oir1Nh"
   },
   "source": [
    "### Make 2 functions to help analyze categorical and continuos data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHY4WMSYry1V"
   },
   "outputs": [],
   "source": [
    "# Analyse categorical features.\n",
    "# the Dataframe with Selected Attributes, Selected Column Name , The Figure Size, Title , Rotation\n",
    "def analyseCategoricalData(df, column, fig_size=(12, 7), title=None, rot=90):\n",
    "    # Print Unique Values and Missing Values \n",
    "    print(\"Number of unique values: {}\\n\".format(len(df[column].unique())))\n",
    "    print(\"Number of missing values: {}\\n\".format(df[column].isnull().sum()))\n",
    "    # Count the Missing Values\n",
    "    df = np.round(df[column].value_counts(normalize=True, ascending=False, dropna=False) * 100, 2)\n",
    "    if True in df.index.isnull():\n",
    "        df.index = df.index.fillna(\"Missing Values\") \n",
    "    print(df)\n",
    "    # Plot the Figure using pyplot \n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    ax = df.plot.bar()\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=rot)\n",
    "    # Save Graph\n",
    "    plt.savefig(column + '.png', dpi=fig.dpi)\n",
    "    plt.show()\n",
    "\n",
    "def analyseContinuousData(df, column, xlabel=\"\", figSize=(12, 7)):\n",
    "    # Print Missing Values and Statistics such as Mean, std,count ....\n",
    "    print(\"Number of missing values: {}\\n\".format(df[column].isnull().sum()))\n",
    "    print('Descriptive statistics:\\n')\n",
    "    print(df[column].describe())\n",
    "\n",
    "    # Plot the graphs Make it as density values to better represent it\n",
    "    fig, ax = plt.subplots()\n",
    "    df.loc[df.DELINQUENT == True, column].plot.hist(bins=50, density=True, ax=ax, alpha=0.4, label='Delinquent')\n",
    "    df.loc[df.DELINQUENT == False, column].plot.hist(bins=50, density=True, ax=ax, alpha=0.4, label='Non-Delinquent')\n",
    "    plt.title(\"Delinquent vs. Non-Delinquent\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Density Value')\n",
    "    plt.legend()\n",
    "    # Save Graph\n",
    "    plt.savefig(column + '.png', dpi=fig.dpi)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPCZcGFTt__Q"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = pandas_df.corr()\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(10, 8))  # Set the size of the heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Attributes')\n",
    "plt.ylabel('Attributes')\n",
    "plt.title('Correlation Heatmap')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWYCrOQBuFOq"
   },
   "source": [
    "### Explore CREDIT_SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHrL2FHJsFBF",
    "outputId": "0ab035ab-fbae-4289-f6a2-652c58fc5ce9"
   },
   "outputs": [],
   "source": [
    "# A credit score is unavailable when its value is 9999.\n",
    "print('The number of loan records with unavailable credit scores:', df.filter(df['CREDIT_SCORE'] == 9999).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rB7GdH5_uI6_"
   },
   "outputs": [],
   "source": [
    "# Replace the values of unavailable credit scores with None.\n",
    "df = df.withColumn('CREDIT_SCORE', when(df['CREDIT_SCORE'] != 9999, df['CREDIT_SCORE']).otherwise(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "mK4okTa_uI9T",
    "outputId": "34ff9e6c-a317-421c-bacb-6cf2e5473b28"
   },
   "outputs": [],
   "source": [
    "# Analyse the CREDIT_SCORE column.\n",
    "pandasDF = df.select(df['CREDIT_SCORE'], df['DELINQUENT']).toPandas()\n",
    "analyseContinuousData(pandasDF, column='CREDIT_SCORE', xlabel='Credit Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZi5U-KDuI_K",
    "outputId": "3f2624bb-1411-443d-8de0-0a481bfc8d3f"
   },
   "outputs": [],
   "source": [
    "# Replace the values of unavailable credit scores with 700.\n",
    "df = df.withColumn('CREDIT_SCORE', when(df['CREDIT_SCORE'].isNotNull(), df['CREDIT_SCORE']).otherwise(700.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('CREDIT_SCORE_CLASS',\n",
    "                   when(df['CREDIT_SCORE'].between(800, 850), 'Excellent')\n",
    "                   .when(df['CREDIT_SCORE'].between(740, 799), 'Very Good')\n",
    "                   .when(df['CREDIT_SCORE'].between(670, 739), 'Good')\n",
    "                   .when(df['CREDIT_SCORE'].between(580, 669), 'Fair')\n",
    "                   .when(df['CREDIT_SCORE'].between(300, 579), 'Poor')\n",
    "                   .otherwise('Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values and their frequency in the 'attribute' column\n",
    "unique_values_count = df.groupBy('CREDIT_SCORE_CLASS').agg(count('*').alias('count'))\n",
    "# Display the result\n",
    "unique_values_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasDF = df.select(df['CREDIT_SCORE_CLASS'], df['DELINQUENT']).toPandas()\n",
    "analyseCategoricalData(\n",
    "    pandasDF,\n",
    "    column='CREDIT_SCORE_CLASS',\n",
    "    fig_size=(8, 6),\n",
    "    title='Credit Score Class',\n",
    "    rot=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(pandasDF.CREDIT_SCORE_CLASS, pandasDF.DELINQUENT, normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gejh7HXzvR3"
   },
   "source": [
    "### Explore First Time Homebuyer Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPKgIwNquJBM",
    "outputId": "77735b8c-3c53-4709-f6d9-4b1d2b55c3d5"
   },
   "outputs": [],
   "source": [
    "# A first-time homebuyer flag is unavailable when its value is '9'.\n",
    "\n",
    "print('The number of loan records with unavailable first-time homebuyer flags:', df.filter(df['FIRST_TIME_HOMEBUYER_FLAG'] == '9').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "id": "HvZvXCSeuJDL",
    "outputId": "80613eb4-1f21-4a04-fb6c-cd1c08ab8a5a"
   },
   "outputs": [],
   "source": [
    "# Analyse the FIRST_TIME_HOMEBUYER_FLAG column.\n",
    "pandasDF = df.select(df['FIRST_TIME_HOMEBUYER_FLAG'], df['DELINQUENT']).toPandas()\n",
    "analyseCategoricalData(\n",
    "    pandasDF,\n",
    "    column='FIRST_TIME_HOMEBUYER_FLAG',\n",
    "    fig_size=(8, 6),\n",
    "    title='First Time Homebuyer Flag',\n",
    "    rot=0\n",
    ")\n",
    "pd.crosstab(pandasDF.FIRST_TIME_HOMEBUYER_FLAG, pandasDF.DELINQUENT, normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Original Interest Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of loan records with unavailable ORIGINAL_INTEREST_RATE:', df.filter(df['ORIGINAL_INTEREST_RATE'].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the ORIGINAL_INTEREST_RATE column.\n",
    "pandasDF = df.select(df['ORIGINAL_INTEREST_RATE'], df['DELINQUENT']).toPandas()\n",
    "analyseContinuousData(pandasDF, column='ORIGINAL_INTEREST_RATE', xlabel='ORIGINAL_INTEREST_RATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Explore METROPOLITAN_STATISTICAL_AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XY7-bIUOuJFQ",
    "outputId": "a56fbc4a-1a6d-4801-93a1-cef90e4bb2f6"
   },
   "outputs": [],
   "source": [
    "# MSA is codes that corresponds to a certain area and have some information about them\n",
    "print('Number of unique MSA codes:', df.select(df['METROPOLITAN_STATISTICAL_AREA']).distinct().count())\n",
    "print('Number of loan records with unavailable MSA codes:', df.filter(df['METROPOLITAN_STATISTICAL_AREA'].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x689hetId7o"
   },
   "source": [
    "replace null with 0 and keep non null as usuall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xB6gl9VVuJHQ"
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('METROPOLITAN_STATISTICAL_AREA', when(df['METROPOLITAN_STATISTICAL_AREA'].isNotNull(), df['METROPOLITAN_STATISTICAL_AREA']).otherwise(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "id": "3LkPgQ5huJJW",
    "outputId": "87d79eed-5a87-42b1-db6a-73f39737d2d7"
   },
   "outputs": [],
   "source": [
    "pandasDF = df.select(df['METROPOLITAN_STATISTICAL_AREA'], df['DELINQUENT']).toPandas()\n",
    "analyseContinuousData(pandasDF, column='METROPOLITAN_STATISTICAL_AREA', xlabel='MSA')\n",
    "pd.crosstab(pandasDF.METROPOLITAN_STATISTICAL_AREA, pandasDF.DELINQUENT, normalize='index').sort_values(by=1, ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5QWXCdE88p7"
   },
   "source": [
    "### Explore NUMBER_OF_UNITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtxI4GbvuJLi",
    "outputId": "a1c8bbd5-5f60-4a6a-c890-0097a44f1819"
   },
   "outputs": [],
   "source": [
    "# The number of units is unavailable when its value is 99.\n",
    "print('The number of loan records with the unavailable number of units:', df.filter(df['NUMBER_OF_UNITS'] == 99).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "id": "HBAIHz4VuJNr",
    "outputId": "93e5b6dc-a44e-41a8-e39a-9f79d6a696d6"
   },
   "outputs": [],
   "source": [
    "# Analyse the NUMBER_OF_UNITS column.\n",
    "\n",
    "pandasDF = df.select(df['NUMBER_OF_UNITS'], df['DELINQUENT']).toPandas()\n",
    "analyseCategoricalData(\n",
    "    pandasDF,\n",
    "    column='NUMBER_OF_UNITS',\n",
    "    fig_size=(8, 6),\n",
    "    title='Number of Units',\n",
    "    rot=0\n",
    ")\n",
    "pd.crosstab(pandasDF.NUMBER_OF_UNITS, pandasDF.DELINQUENT, normalize='index').sort_values(by=1, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N70snvVc9AQd"
   },
   "source": [
    "### Explore OCCUPANCY_STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvN-JLAPuJPv",
    "outputId": "5ae91bd7-5951-4771-c0bd-187df34e3126"
   },
   "outputs": [],
   "source": [
    "# An occupancy status is unavailable when its value is '9'.\n",
    "print('The number of loan records with unavailable occupancy status:', df.filter(df['OCCUPANCY_STATUS'] == '9').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 841
    },
    "id": "cd3dJuisuJRy",
    "outputId": "64f106f8-034c-4227-946a-d69e6c969f4e"
   },
   "outputs": [],
   "source": [
    "# Analyse the OCCUPANCY_STATUS column.\n",
    "pandasDF = df.select(df['OCCUPANCY_STATUS'], df['DELINQUENT']).toPandas()\n",
    "analyseCategoricalData(\n",
    "    pandasDF,\n",
    "    column='OCCUPANCY_STATUS',\n",
    "    fig_size=(8, 6),\n",
    "    title='Occupancy Status',\n",
    "    rot=0\n",
    ")\n",
    "pd.crosstab(pandasDF.OCCUPANCY_STATUS, pandasDF.DELINQUENT, normalize='index').sort_values(by=1, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCzeDGH59ERn"
   },
   "source": [
    "### Explore ORIGINAL_COMBINED_LOAN_TO_VALUE of property\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LSXnw0TkuJT-",
    "outputId": "0c1852aa-f69b-4efd-cb98-c9a0ff4850da"
   },
   "outputs": [],
   "source": [
    "# An original combined LTV is unavailable when its value is 999.\n",
    "print('The number of loan records with unavailable original combined LTVs:', df.filter(df['ORIGINAL_COMBINED_LOAN_TO_VALUE'] == 999).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9kPTCIAuJV7"
   },
   "outputs": [],
   "source": [
    "# Replace unavailable values with None for the EDA\n",
    "df = df.withColumn('ORIGINAL_COMBINED_LOAN_TO_VALUE', when(df['ORIGINAL_COMBINED_LOAN_TO_VALUE'] != 999, df['ORIGINAL_COMBINED_LOAN_TO_VALUE']).otherwise(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "iEYT0BLBuJYi",
    "outputId": "e016ba78-ab8a-47a1-ad62-f5d7ce9c2077"
   },
   "outputs": [],
   "source": [
    "# Analyse the ORIGINAL_COMBINED_LOAN_TO_VALUE column.\n",
    "pandasDF = df.select(df['ORIGINAL_COMBINED_LOAN_TO_VALUE'], df['DELINQUENT']).toPandas()\n",
    "analyseContinuousData(pandasDF, column='ORIGINAL_COMBINED_LOAN_TO_VALUE', xlabel='Original Combined Loan-to-Value (LTV)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mP7nGHlPuJah"
   },
   "outputs": [],
   "source": [
    "# Replace the missing values with the median for training\n",
    "df = df.withColumn('ORIGINAL_COMBINED_LOAN_TO_VALUE', when(df['ORIGINAL_COMBINED_LOAN_TO_VALUE'].isNotNull(), df['ORIGINAL_COMBINED_LOAN_TO_VALUE']).otherwise(80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfVJ8uAK9IvG"
   },
   "source": [
    "### Explore ORIGINAL_DEBT_TO_INCOME_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ii15y7imuJdN",
    "outputId": "bf0a7c6b-3c9d-432c-a455-769b279e761d"
   },
   "outputs": [],
   "source": [
    "# An original combined LTV is unavailable when its value is 999.\n",
    "print('The number of loan records with unavailable original DTIR:', df.filter(df['ORIGINAL_DEBT_TO_INCOME_RATIO'] == 999).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ao5z7xniuJfe"
   },
   "outputs": [],
   "source": [
    "# Replace unavailable values with None.\n",
    "df = df \\\n",
    ".withColumn('ORIGINAL_DEBT_TO_INCOME_RATIO', when(df['ORIGINAL_DEBT_TO_INCOME_RATIO'] != 999, df['ORIGINAL_DEBT_TO_INCOME_RATIO']).otherwise(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "_uYD5dUguJhp",
    "outputId": "1e9937f4-a465-4056-d35a-b301f898ae1a"
   },
   "outputs": [],
   "source": [
    "# Analyse the ORIGINAL_DEBT_TO_INCOME_RATIO column.\n",
    "pandasDF = df.select(df['ORIGINAL_DEBT_TO_INCOME_RATIO'], df['DELINQUENT']).toPandas()\n",
    "analyseContinuousData(pandasDF, column='ORIGINAL_DEBT_TO_INCOME_RATIO', xlabel='Original Debt-to-Income (DTI) Ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuWa4LxauJju"
   },
   "outputs": [],
   "source": [
    "# Replace unavailable values with 999 to check the density values\n",
    "df = df \\\n",
    ".withColumn('ORIGINAL_DEBT_TO_INCOME_RATIO', when(df['ORIGINAL_DEBT_TO_INCOME_RATIO'].isNotNull(), df['ORIGINAL_DEBT_TO_INCOME_RATIO']).otherwise(999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTfczy6quJlx"
   },
   "outputs": [],
   "source": [
    "# Create a new colomn named ORIGINAL_DEBT_TO_INCOME_RATIO_Class so it will not mix\n",
    "df = df.withColumn('ORIGINAL_DEBT_TO_INCOME_RATIO_Class', when((df['ORIGINAL_DEBT_TO_INCOME_RATIO'] >= 35) , \"High\").otherwise(\"low\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "LIdNDdO1uJoF",
    "outputId": "1c934f25-cd1a-4698-d70a-98a0a94cd866"
   },
   "outputs": [],
   "source": [
    "# Analyse the ORIGINAL_DEBT_TO_INCOME_RATIO_Class colomn.\n",
    "pandasDF = df.select(df['ORIGINAL_DEBT_TO_INCOME_RATIO_Class'], df['DELINQUENT']).toPandas()\n",
    "analyseCategoricalData(\n",
    "    pandasDF,\n",
    "    column='ORIGINAL_DEBT_TO_INCOME_RATIO_Class',\n",
    "    fig_size=(8, 6),\n",
    "    title='ORIGINAL_DEBT_TO_INCOME_RATIO_Class',\n",
    "    rot=0\n",
    ")\n",
    "pd.crosstab(pandasDF.ORIGINAL_DEBT_TO_INCOME_RATIO_Class, pandasDF.DELINQUENT, normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values and their frequency in the 'attribute' column\n",
    "unique_values_count = df.groupBy('ORIGINAL_DEBT_TO_INCOME_RATIO_Class').agg(count('*').alias('count'))\n",
    "\n",
    "# Display the result\n",
    "unique_values_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvLBrWPyMw_h"
   },
   "source": [
    "### Explore ORIGINAL_LOAN_TO_VALUE of the property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDPc9V7F9OrL",
    "outputId": "ebfb0a20-8812-438d-a764-e5d9a4f20593"
   },
   "outputs": [],
   "source": [
    "# An original LTV is unavailable when its value is 999.\n",
    "print('The number of loan records with unavailable original LTVs:', df.filter(df['ORIGINAL_LOAN_TO_VALUE'] == 999).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtiSdIDW9OtV"
   },
   "outputs": [],
   "source": [
    "# Replace unavailable values with None.\n",
    "df = df.withColumn('ORIGINAL_LOAN_TO_VALUE', when(df['ORIGINAL_LOAN_TO_VALUE'] != 999, df['ORIGINAL_LOAN_TO_VALUE']).otherwise(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "hve5_roG9Ovk",
    "outputId": "6f81f22d-47f9-487c-e164-0647ad8265be"
   },
   "outputs": [],
   "source": [
    "# Analyse the ORIGINAL_LOAN_TO_VALUE column.\n",
    "pandasDF = df.select(df['ORIGINAL_LOAN_TO_VALUE'], df['DELINQUENT']).toPandas()\n",
    "analyseContinuousData(pandasDF, column='ORIGINAL_LOAN_TO_VALUE', xlabel='Original Loan-to-Value (LTV)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLxdfMc39Oxs"
   },
   "outputs": [],
   "source": [
    "# Replace  the missing values with the median.\n",
    "df = df.withColumn('ORIGINAL_LOAN_TO_VALUE', when(df['ORIGINAL_LOAN_TO_VALUE'].isNotNull(), df['ORIGINAL_LOAN_TO_VALUE']).otherwise(80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU-WEo1-M0kP"
   },
   "source": [
    "### Explore CHANNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 913
    },
    "id": "oP6a6P5yM3f7",
    "outputId": "0f028609-2c8e-4f93-814a-ab53b0be22a6"
   },
   "outputs": [],
   "source": [
    "# Analyse the CHANNEL column. \n",
    "pandasDF = df.select(df['CHANNEL'], df['DELINQUENT']).toPandas()\n",
    "analyseCategoricalData(pandasDF, column='CHANNEL', title='Channel')\n",
    "pd.crosstab(pandasDF.CHANNEL, pandasDF.DELINQUENT, normalize='index').sort_values(by=1, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr7fHtPQNIRL"
   },
   "source": [
    "### Explore PROPERTY_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp61IfbJsSpS"
   },
   "source": [
    "SF = Single Family\n",
    "CO Condomonium\n",
    "MH Manufactured Housing\n",
    "CP: Commercial Property\n",
    "PU: Planned Unit Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iN4cCrQCM3ia",
    "outputId": "d1f899c5-2274-4e36-d307-a2e6852ce603"
   },
   "outputs": [],
   "source": [
    "# Analyse the PROPERTY_TYPE #SIngle Family,.\n",
    "\n",
    "pandasDF = df.select(df['PROPERTY_TYPE'], df['DELINQUENT']).toPandas()\n",
    "analyseCategoricalData(pandasDF, column='PROPERTY_TYPE',title='Property Type')\n",
    "pd.crosstab(pandasDF.PROPERTY_TYPE, pandasDF.DELINQUENT, normalize='index').sort_values(by=1, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz3xcTerNJYB"
   },
   "source": [
    "### Explore POSTAL_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKX_mh9VM3kg",
    "outputId": "94ea6a06-3165-406f-d454-90b6aa0e9515"
   },
   "outputs": [],
   "source": [
    "print('Number of loan records with unavailable postal codes:', df.filter(df['POSTAL_CODE'].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcTVgv96NNuZ"
   },
   "source": [
    "### Explore LOAN_PURPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 914
    },
    "id": "Jv1Uw3jNM3mm",
    "outputId": "1e5e8e50-b41c-452a-faed-2c56c828126e"
   },
   "outputs": [],
   "source": [
    "# Analyse the LOAN_PURPOSE column\n",
    "pandasDF = df.select(df['LOAN_PURPOSE'], df['DELINQUENT']).toPandas()\n",
    "analyseCategoricalData(pandasDF, column='LOAN_PURPOSE',title='Loan Purpose')\n",
    "pd.crosstab(pandasDF.LOAN_PURPOSE, pandasDF.DELINQUENT, normalize='index').sort_values(by=1, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_R9HN9eUNQab"
   },
   "source": [
    "### Explore ORIGINAL_LOAN_TERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "SOtUGU7xM3ov",
    "outputId": "23a18cdc-de0d-452d-fbb4-8fc348a2884b"
   },
   "outputs": [],
   "source": [
    "# Analyse the ORIGINAL_LOAN_TERM column (Months).\n",
    "pandasDF = df.select(df['ORIGINAL_LOAN_TERM'], df['DELINQUENT']).toPandas()\n",
    "analyseContinuousData(pandasDF, column='ORIGINAL_LOAN_TERM',xlabel='Original Loan Term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rlxza8xVOe9r"
   },
   "outputs": [],
   "source": [
    "# Replace mIssing with mean of 327\n",
    "df = df.withColumn('ORIGINAL_LOAN_TERM', when(df['ORIGINAL_LOAN_TERM'].isNotNull(), df['ORIGINAL_LOAN_TERM']).otherwise(327))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1nvVPdQNRnB"
   },
   "source": [
    "### Explore NUMBER_OF_BORROWERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30bPhOnIM3qt",
    "outputId": "79cdd1b8-81a8-451f-b365-b032e3830106"
   },
   "outputs": [],
   "source": [
    "# The number of borrowers is unavailable when its value is 99.\n",
    "print('The number of loan records with unavailable borrower numbers:', df.filter(df['NUMBER_OF_BORROWERS'] == 99).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rLXDaCbBNUWF",
    "outputId": "2328c2b9-c198-447c-a889-e9ddb08f1b9c"
   },
   "outputs": [],
   "source": [
    "df.select(df['NUMBER_OF_BORROWERS']).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7utfTxlNUYP"
   },
   "outputs": [],
   "source": [
    "# Cast its data type to StringType.\n",
    "df = df.withColumn('NUMBER_OF_BORROWERS', df['NUMBER_OF_BORROWERS'].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oYiEywCHNUaP",
    "outputId": "cbbfea1b-5283-4a27-ddc5-8496dfacdd3f"
   },
   "outputs": [],
   "source": [
    "# Analyse the NUMBER_OF_BORROWERS column.\n",
    "pandasDF = df.select(df['NUMBER_OF_BORROWERS'], df['DELINQUENT']).toPandas()\n",
    "analyseCategoricalData(pandasDF, column='NUMBER_OF_BORROWERS',title='Number of Borrowers')\n",
    "pd.crosstab(pandasDF.NUMBER_OF_BORROWERS, pandasDF.DELINQUENT, normalize='index').sort_values(by=1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZyIN4_ZNUcS"
   },
   "outputs": [],
   "source": [
    "# Cast its data type back to DoubleType.\n",
    "df = df.withColumn('NUMBER_OF_BORROWERS', df['NUMBER_OF_BORROWERS'].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBeewldMP9t6"
   },
   "source": [
    "##  EDA done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dz-AlFJMNUeI",
    "outputId": "445b9cc4-7507-4da0-80e2-1f73332432f5"
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check amount of Denlinquents and Non Delinquents, OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check amount of Denlinquents and Non Delinquents, OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into positive (DELINQUENT = 1) and negative (DELINQUENT = 0) examples\n",
    "positive_df = df.filter(col(\"DELINQUENT\") == 1)\n",
    "negative_df = df.filter(col(\"DELINQUENT\") == 0)\n",
    "print(positive_df.count())\n",
    "print(negative_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratio of positive to negative examples\n",
    "positive_count = positive_df.count()\n",
    "negative_count = negative_df.count()\n",
    "oversample_ratio = negative_count / positive_count\n",
    "print(oversample_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a high ratio of positive to negative, but for this scenario 2 should be enough\n",
    "oversample_ratio = 2.0\n",
    "# Oversample the positive examples\n",
    "oversampled_positive_df = positive_df.sample(withReplacement=True, fraction=oversample_ratio, seed=42)\n",
    "\n",
    "# Combine the oversampled positive examples with the original negative examples\n",
    "balanced_df = negative_df.unionAll(oversampled_positive_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_positive_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Gradient Boosting Decision Tree Model - Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since PySpark have problems, Change to Pandas Dataframe to use with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = balanced_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = pandas_df.shape\n",
    "print(\"shape:\", shape)\n",
    "schema = pandas_df.dtypes\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = pandas_df.columns[pandas_df.isnull().any()]\n",
    "if null_columns.empty:\n",
    "    print(\"No null values found in any column.\")\n",
    "else:\n",
    "    print(\"Null values found in the following columns:\")\n",
    "    for col in null_columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {}\n",
    "# Label Encode And Save the Label Encoder \n",
    "label_encoder = LabelEncoder()\n",
    "categorical_cols = [\"FIRST_TIME_HOMEBUYER_FLAG\", \"OCCUPANCY_STATUS\", \"CHANNEL\", \"PROPERTY_TYPE\", \"LOAN_PURPOSE\"]\n",
    "for col in categorical_cols:\n",
    "    pandas_df[col] = label_encoder.fit_transform(pandas_df[col])\n",
    "    # Create a mapping dictionary\n",
    "    mapping = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
    "\n",
    "    # Save the mapping dictionary to a CSV file\n",
    "    mapping_df = pd.DataFrame.from_dict(mapping, orient='index', columns=[col])\n",
    "    mapping_df.to_csv(f\"{col}_mapping.csv\", index_label='Encoded_Value')\n",
    "\n",
    "    # Store the mapping dictionary in the mappings dictionary\n",
    "    mappings[col] = mapping\n",
    "    \n",
    "    joblib.dump(label_encoder, f\"{col}_mapping.joblib\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    [450, 'N', 12345.0, 1, 'I', 74,20,10,2, 'R', 'SF', 54321.0, 'P', 360.0, 1.0],\n",
    "    [200, 'Y', 23456.0, 2, 'P', 115,30, 100,3, 'C', 'CO', 65432.0, 'N', 240.0, 2.0],\n",
    "    [650, 'N', 34567.0, 1, 'S', 100,40, 30,4, 'B', 'CP', 76543.0, 'C', 180.0, 1.0],\n",
    "    [500, 'Y', 45678.0, 2, 'I', 50,50, 40, 5,'B', 'MH', 87654.0, 'C', 300.0, 3.0],\n",
    "    [500, 'Y', 47664.0, 2, 'P', 69,10, 69,6, 'C', 'SF', 48300.0, 'N', 240.0, 1.0],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names based on the given schema\n",
    "columns = [\n",
    "    'CREDIT_SCORE',\n",
    "    'FIRST_TIME_HOMEBUYER_FLAG',\n",
    "    'METROPOLITAN_STATISTICAL_AREA',\n",
    "    'NUMBER_OF_UNITS', \n",
    "    'OCCUPANCY_STATUS',\n",
    "    'ORIGINAL_COMBINED_LOAN_TO_VALUE',\n",
    "    'ORIGINAL_DEBT_TO_INCOME_RATIO',\n",
    "    'ORIGINAL_LOAN_TO_VALUE',\n",
    "    'ORIGINAL_INTEREST_RATE',\n",
    "    'CHANNEL',\n",
    "    'PROPERTY_TYPE',\n",
    "    'POSTAL_CODE',\n",
    "    'LOAN_PURPOSE',\n",
    "    'ORIGINAL_LOAN_TERM',\n",
    "    'NUMBER_OF_BORROWERS',\n",
    "    \n",
    "]\n",
    "test_DF = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    label_encoder = joblib.load(f\"{col}_mapping.joblib\")\n",
    "    test_DF[col] = label_encoder.fit_transform(test_DF[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label Encoder Working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split Data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pandas_df.drop(\"DELINQUENT\", axis=1)\n",
    "y = pandas_df[\"DELINQUENT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Decision Tree - Filled with Best Parameters from GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier\n",
    "dtModel = DecisionTreeClassifier(\n",
    "    criterion='gini',  \n",
    "    max_depth=20,       # Maximum depth of the tree\n",
    "    min_samples_split=2,  # Minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=1,   # Minimum number of samples required to be at a leaf node\n",
    "    random_state=42      # Random seed for reproducibility\n",
    ")\n",
    "start_time = datetime.now()\n",
    "# Train the decision tree classifier\n",
    "dtModel.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(\"Training time = %s\" % training_time)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "testpreds = dtModel.predict(X_test)\n",
    "trainpreds = dtModel.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute false positive rate, true positive rate, and thresholds\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, testpreds)\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, trainpreds)\n",
    "\n",
    "# Compute AUC-ROC for training and testing data\n",
    "auc_roc_train = auc(fpr_train, tpr_train)\n",
    "auc_roc_test = auc(fpr_test, tpr_test)\n",
    "print(auc_roc_train)\n",
    "print(auc_roc_test)\n",
    "\n",
    "# Plot the ROC curves for training and testing data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_train, tpr_train, label='Train ROC curve (AUC = %0.2f)' % auc_roc_train)\n",
    "plt.plot(fpr_test, tpr_test, label='Test ROC curve (AUC = %0.2f)' % auc_roc_test)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Decision Tree ROC Graph ')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for Test\n",
    "accuracy = accuracy_score(y_test, testpreds)\n",
    "precision = precision_score(y_test, testpreds)\n",
    "recall = recall_score(y_test, testpreds)\n",
    "f1 = f1_score(y_test, testpreds)\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion = confusion_matrix(y_test, testpreds)\n",
    "confusion_df = pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "\n",
    "# Print evaluation metrics and confusion matrix\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for Train\n",
    "accuracy = accuracy_score(y_train, trainpreds)\n",
    "precision = precision_score(y_train, trainpreds)\n",
    "recall = recall_score(y_train, trainpreds)\n",
    "f1 = f1_score(y_train, trainpreds)\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion = confusion_matrix(y_train, trainpreds)\n",
    "confusion_df = pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "\n",
    "# Print evaluation metrics and confusion matrix\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Random Forest - Filled with Best Parameters from GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier with specified parameters\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,    # Number of decision trees in the forest\n",
    "    criterion='gini',    # Split criterion ('gini' or 'entropy')\n",
    "    max_depth=20,      # Maximum depth of the trees\n",
    "    min_samples_split=2, # Minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=1,  # Minimum number of samples required to be at a leaf node\n",
    "    random_state=42      # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "start_time = datetime.now()\n",
    "# Train the Random Forest classifier\n",
    "clf.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "training_time = end_time - start_time\n",
    "print(\"Training time = %s\" % training_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "testpreds = clf.predict(X_test)\n",
    "trainpreds = clf.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute false positive rate, true positive rate, and thresholds\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, testpreds)\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, trainpreds)\n",
    "\n",
    "# Compute AUC-ROC for training and testing data\n",
    "auc_roc_train = auc(fpr_train, tpr_train)\n",
    "auc_roc_test = auc(fpr_test, tpr_test)\n",
    "print(auc_roc_train)\n",
    "print(auc_roc_test)\n",
    "\n",
    "# Plot the ROC curves for training and testing data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_train, tpr_train, label='Train ROC curve (AUC = %0.2f)' % auc_roc_train)\n",
    "plt.plot(fpr_test, tpr_test, label='Test ROC curve (AUC = %0.2f)' % auc_roc_test)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Random Forest ROC Graph ')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for Test\n",
    "accuracy = accuracy_score(y_test, testpreds)\n",
    "precision = precision_score(y_test, testpreds)\n",
    "recall = recall_score(y_test, testpreds)\n",
    "f1 = f1_score(y_test, testpreds)\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion = confusion_matrix(y_test, testpreds)\n",
    "confusion_df = pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "\n",
    "# Print evaluation metrics and confusion matrix\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for Train\n",
    "accuracy = accuracy_score(y_train, trainpreds)\n",
    "precision = precision_score(y_train, trainpreds)\n",
    "recall = recall_score(y_train, trainpreds)\n",
    "f1 = f1_score(y_train, trainpreds)\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion = confusion_matrix(y_train, trainpreds)\n",
    "confusion_df = pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "\n",
    "# Print evaluation metrics and confusion matrix\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBClassifier Decision Tree With Best Params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'colsample_bytree': 1.0,\n",
    " 'learning_rate': 0.1,\n",
    " 'max_depth': 15,\n",
    " 'n_estimators': 350,\n",
    " 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "clf = xgb.XGBClassifier(**best_params)\n",
    "clf.fit(X_train, y_train.astype(int))\n",
    "end_time = datetime.now()\n",
    "training_time = end_time - start_time\n",
    "print(\"Training time = %s\" % training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "testpreds = clf.predict(X_test)\n",
    "trainpreds = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute false positive rate, true positive rate, and thresholds\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, testpreds)\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, trainpreds)\n",
    "\n",
    "# Compute AUC-ROC for training and testing data\n",
    "auc_roc_train = auc(fpr_train, tpr_train)\n",
    "auc_roc_test = auc(fpr_test, tpr_test)\n",
    "print(auc_roc_train)\n",
    "print(auc_roc_test)\n",
    "\n",
    "# Plot the ROC curves for training and testing data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_train, tpr_train, label='Train ROC curve (AUC = %0.2f)' % auc_roc_train)\n",
    "plt.plot(fpr_test, tpr_test, label='Test ROC curve (AUC = %0.2f)' % auc_roc_test)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('XGBoost ROC Graph ')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate evaluation metrics for test\n",
    "accuracy = accuracy_score(y_test, testpreds)\n",
    "precision = precision_score(y_test, testpreds)\n",
    "recall = recall_score(y_test, testpreds)\n",
    "f1 = f1_score(y_test, testpreds)\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion = confusion_matrix(y_test, testpreds)\n",
    "confusion_df = pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "\n",
    "# Print evaluation metrics and confusion matrix\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for Train\n",
    "accuracy = accuracy_score(y_train, trainpreds)\n",
    "precision = precision_score(y_train, trainpreds)\n",
    "recall = recall_score(y_train, trainpreds)\n",
    "f1 = f1_score(y_train, trainpreds)\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion = confusion_matrix(y_train, trainpreds)\n",
    "confusion_df = pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "\n",
    "# Print evaluation metrics and confusion matrix\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too Big Plot Decision Tree\n",
    "# plot_tree(clf, rankdir='LR', num_trees=1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf.save_model(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance = clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print or display the feature importance\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for best parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [5, 10, 15,20],\n",
    "    'min_samples_split': [2, 5, 10,15],\n",
    "    'min_samples_leaf': [1, 2, 4,8]\n",
    "}\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Decision Tree classifier and perform grid search\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_grid_search = GridSearchCV(dt_clf, dt_param_grid, cv=kfold)\n",
    "\n",
    "start_time = datetime.now()\n",
    "dt_grid_search.fit(X_train, y_train)\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_time = end_time - start_time\n",
    "print(\"Training time = %s\" % training_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best Decision Tree model and evaluate on the test set\n",
    "dt_best_model = dt_grid_search.best_estimator_\n",
    "dt_predictions = dt_best_model.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "print(\"Decision Tree Accuracy:\", dt_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predictions = clf.predict(X_test)\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, dt_predictions)\n",
    "precision = precision_score(y_test, dt_predictions)\n",
    "recall = recall_score(y_test, dt_predictions)\n",
    "f1 = f1_score(y_test, dt_predictions)\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion = confusion_matrix(y_test, dt_predictions)\n",
    "confusion_df = pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "\n",
    "# Print evaluation metrics and confusion matrix\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Best Parameters\n",
    "best_params = dt_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [5, 10,15,20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier and perform grid search\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_grid_search = GridSearchCV(rf_clf, rf_param_grid, cv=kfold)\n",
    "start_time = datetime.now()\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "training_time = end_time - start_time\n",
    "print(\"Training time = %s\" % training_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best Random Forest model and evaluate on the test set\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "rf_predictions = rf_best_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = rf_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the Parameters for XGBoost\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7,9,11,13,15],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [100,150, 200,250, 300,350,400],\n",
    "    'subsample': [0.8, 1.0,1.2,1.4],\n",
    "    'colsample_bytree': [0.8, 1.0,1.2,1.4]\n",
    "}\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "model = xgb.XGBClassifier()\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='accuracy', cv=kfold)\n",
    "grid_search.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "training_time = end_time - start_time\n",
    "print(\"Training time = %s\" % training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "precision = precision_score(y_test, preds)\n",
    "recall = recall_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds)\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion = confusion_matrix(y_test, preds)\n",
    "confusion_df = pd.DataFrame(confusion, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "\n",
    "# Print evaluation metrics and confusion matrix\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance = best_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print or display the feature importance\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
